# M9.5: ML-Powered Parsing - PRD

**Project**: Forager - Smart Meal Planner  
**Milestone**: M9.5 (Optional Enhancement to M9 Health & Nutrition)  
**Status**: ‚è≥ PLANNED (OPTIONAL)  
**Version**: 1.0  
**Created**: December 19, 2025  
**Dependencies**: M8 complete (substantial training data from user corrections)

---

## ‚ö†Ô∏è IMPORTANT: OPTIONAL MILESTONE

**M9.5 is OPTIONAL** - Only pursue if:
- ‚úÖ You have 100+ user corrections from M7-M8 usage
- ‚úÖ You want industry-leading parsing (98% ‚Üí 99.5%+ accuracy)
- ‚úÖ You have 15-20 hours available
- ‚úÖ You want to showcase ML expertise in portfolio

**Not pursuing M9.5 is perfectly valid.** The hybrid NLP system from M8.0 already achieves professional-grade 98%+ accuracy.

---

## Executive Summary

### What is M9.5?

M9.5 implements a **custom Create ML model** trained on real user corrections to achieve industry-leading ingredient parsing accuracy (99.5%+) with on-device inference for maximum privacy.

**Core Components:**
- Training dataset from 100+ user corrections (M7-M8 telemetry)
- Create ML text classifier for ingredient parsing
- On-device inference using CoreML
- Continuous learning from ongoing corrections

### Why ML?

**Problem**: Even hybrid NLP (M8.0) has limits
- Complex patterns: "1 lb chicken breast, cut into 1-inch cubes"
- Regional variations: "aubergine" vs "eggplant"
- Recipe-specific jargon: "soffritto", "mirepoix"
- User-specific patterns: your app's unique user base

**Solution**: Custom ML model learns from actual usage
- Trained on YOUR users' actual corrections
- Adapts to YOUR users' specific patterns
- Self-improving as more data collected
- Privacy-preserving (on-device inference)

### Why Optional?

**ROI Analysis:**

**Investment**: 15-20 hours development time

**Returns**:
- Accuracy: 98% ‚Üí 99.5% (+1.5% absolute)
- Edit rate: 2% ‚Üí 0.5% (-75% relative)
- Portfolio value: Showcases ML expertise
- Competitive edge: Best-in-class parsing

**Trade-off**: Diminishing returns‚Äî98% is already professional-grade

**Decision**: Only worth it if you want **best-in-class** or need **portfolio showcase**

### Key Outcomes

By the end of M9.5 (if pursued):
- ‚úÖ Custom CoreML model trained on real user data
- ‚úÖ On-device inference (privacy-first)
- ‚úÖ 99.5%+ parsing accuracy on YOUR users' inputs
- ‚úÖ Self-improving system (learns from corrections)
- ‚úÖ Industry-leading capability
- ‚úÖ Strong portfolio piece

---

## Strategic Context

### Evolution from M8.0

**M7.5 Baseline** (95% accuracy):
- Regex parser
- Graceful degradation
- User correction UI

**M8.0 Enhancement** (98% accuracy):
- Hybrid regex + NLP
- Pattern-specific handlers
- Data-driven improvement

**M9.5 ML Upgrade** (99.5%+ accuracy):
- Custom ML model
- Learns from YOUR users
- Self-improving
- Industry-leading

### The ML Advantage

**What ML Adds Beyond NLP:**

1. **Learns User-Specific Patterns**
   - Your users type ingredients differently
   - ML learns THEIR specific patterns
   - Not generic "internet recipes"

2. **Handles Novel Inputs**
   - NLP requires explicit rules
   - ML generalizes from examples
   - Handles variations never seen before

3. **Continuous Improvement**
   - Every correction improves model
   - Re-train monthly/quarterly
   - Gets better over time automatically

4. **Regional/Cultural Adaptation**
   - British users: "aubergine", "courgette"
   - American users: "eggplant", "zucchini"
   - ML learns both from usage

**Example**:
```
User types: "1 medium aubergine, diced into 1cm cubes"

Regex: ‚ùå Fails (doesn't know "aubergine")
NLP: ‚ö†Ô∏è Low confidence (unfamiliar ingredient)
ML: ‚úÖ High confidence (learned from other users' corrections)
```

---

## User Personas

### Primary: Competitive Developer (Rich)

**Background**:
- Building professional portfolio
- Wants "best-in-class" features
- Showcasing ML/AI expertise

**Goal**:
- Stand out in job market
- Demonstrate advanced technical skills
- Show data-driven product development

**M9.5 Value**:
- "Custom ML model trained on real user data"
- "On-device inference for privacy"
- "Self-improving system"
- Portfolio differentiator

### Secondary: Power User (Sarah, Month 3+)

**M8 Experience**:
- 98% accuracy is "pretty good"
- Still occasionally edits complex ingredients
- Notices some patterns app doesn't learn

**M9.5 Experience**:
- App "remembers" her corrections
- Rarely needs to edit anymore
- Feels like app "knows her style"
- Delighted by continuous improvement

---

## Goals & Success Criteria

### Functional Goals

**FG-1: Training Dataset Creation**
- Collect 100+ user corrections from M7-M8 telemetry
- Label with correct parsed components
- Split into train/validate/test sets (70/15/15)
- Format for Create ML consumption

**FG-2: Model Training**
- Use Create ML to train text classifier
- Input: Ingredient string
- Output: Structured components (quantity, unit, name)
- Validate accuracy on test set (‚â•99%)

**FG-3: On-Device Inference**
- Integrate CoreML model into app
- Replace NLP parser with ML parser in hybrid system
- Maintain performance (< 0.2s inference)
- Privacy-preserving (no server calls)

**FG-4: Continuous Learning**
- Log ongoing corrections
- Periodic model retraining (monthly/quarterly)
- A/B testing to validate improvements
- Gradual rollout of updated models

### Non-Functional Goals

**NFG-1: Privacy-First**
- All inference on-device (no cloud)
- Training data anonymized
- Model updates opt-in
- Transparent to users

**NFG-2: Performance Maintained**
- Inference: < 0.2s (same as NLP)
- Model size: < 5MB (reasonable download)
- Memory: < 50MB at runtime
- Battery impact: Negligible

**NFG-3: Accuracy Target**
- Overall: ‚â• 99.5% (vs 98% from M8.0)
- User edit rate: < 0.5% (vs 2% from M8.0)
- User corrections: ‚â•95% learned after 1-2 examples

### Success Criteria

**Must Have (P0):**
- [ ] Training dataset: ‚â•100 labeled examples
- [ ] Model accuracy on test set: ‚â•99%
- [ ] CoreML integration working
- [ ] Inference time: < 0.2s
- [ ] Overall parsing accuracy: ‚â•99.5%
- [ ] User edit rate: < 0.5%
- [ ] Zero privacy leaks
- [ ] Build succeeds with no warnings

**Should Have (P1):**
- [ ] Continuous learning pipeline functional
- [ ] A/B testing framework operational
- [ ] Model update mechanism working
- [ ] User feedback shows "app learns my style"
- [ ] Actual time: 15-20 hours
- [ ] Documentation complete

**Nice to Have (P2):**
- [ ] Multi-language support (Spanish, French, etc.)
- [ ] Transfer learning from larger models
- [ ] Explainability (why did model predict X?)
- [ ] User control over model personalization

---

## Phase Breakdown

### Phase 1: Training Dataset Creation (4-5 hours)

**M9.5.1: Data Collection** (2 hours)

*Goal*: Gather high-quality training data from telemetry

*Implementation*:
```swift
struct TrainingExample: Codable {
    let id: UUID
    let originalText: String
    let correctQuantity: Double?
    let correctUnit: String?
    let correctIngredient: String
    let correctNotes: String?
    let userCorrected: Bool
    let confidence: Float
}

class TrainingDataCollector {
    func collectFromTelemetry() -> [TrainingExample] {
        // Load M7-M8 telemetry
        let telemetry = ParsingTelemetryService.shared.loadAll()
        
        // Filter for user-corrected entries
        let corrections = telemetry.filter { $0.wasManuallyEdited }
        
        // Extract training examples
        return corrections.compactMap { entry in
            guard let corrected = parseCorrectedText(entry.correctedText) else {
                return nil
            }
            
            return TrainingExample(
                id: entry.id,
                originalText: entry.originalText,
                correctQuantity: corrected.quantity,
                correctUnit: corrected.unit,
                correctIngredient: corrected.ingredient,
                correctNotes: corrected.notes,
                userCorrected: true,
                confidence: 1.0  // User corrections are ground truth
            )
        }
    }
    
    func augmentDataset(_ examples: [TrainingExample]) -> [TrainingExample] {
        // Add synthetic variations
        var augmented = examples
        
        for example in examples {
            // Case variations
            augmented.append(caseVariation(example))
            
            // Spelling variations
            augmented.append(spellingVariation(example))
            
            // Order variations (if applicable)
            augmented.append(orderVariation(example))
        }
        
        return augmented
    }
}
```

*Output*:
- `training-data.json`: 100+ examples from user corrections
- `augmented-training-data.json`: 300+ examples with synthetic variations

*Files Created*:
- `MLTraining/TrainingDataCollector.swift`
- `MLTraining/Data/training-data.json`

---

**M9.5.2: Data Labeling & Validation** (1 hour)

*Goal*: Ensure training data is high quality

*Implementation*:
```swift
class DataValidator {
    func validateTrainingData(_ examples: [TrainingExample]) -> ValidationReport {
        var issues: [String] = []
        
        for example in examples {
            // Check for missing fields
            if example.correctIngredient.isEmpty {
                issues.append("Empty ingredient: \(example.id)")
            }
            
            // Check for inconsistencies
            if example.correctQuantity != nil && example.correctUnit == nil {
                issues.append("Quantity without unit: \(example.id)")
            }
            
            // Check for duplicates
            // ... validation logic
        }
        
        return ValidationReport(
            totalExamples: examples.count,
            validExamples: examples.count - issues.count,
            issues: issues
        )
    }
}
```

*Manual Review*:
- Spot-check 20 random examples
- Verify correct labels
- Fix any obvious errors

*Output*:
- `validation-report.md`: Data quality assessment
- `cleaned-training-data.json`: High-quality training set

---

**M9.5.3: Dataset Splitting** (30 min)

*Goal*: Create train/validate/test splits

*Implementation*:
```swift
func splitDataset(_ examples: [TrainingExample]) -> (
    train: [TrainingExample],
    validate: [TrainingExample],
    test: [TrainingExample]
) {
    let shuffled = examples.shuffled()
    
    let trainEnd = Int(Double(shuffled.count) * 0.70)
    let validateEnd = trainEnd + Int(Double(shuffled.count) * 0.15)
    
    let train = Array(shuffled[0..<trainEnd])
    let validate = Array(shuffled[trainEnd..<validateEnd])
    let test = Array(shuffled[validateEnd...])
    
    return (train, validate, test)
}
```

*Output*:
- `train.json`: 70% of data (~210 examples)
- `validate.json`: 15% of data (~45 examples)
- `test.json`: 15% of data (~45 examples)

---

**M9.5.4: Create ML Format Conversion** (30 min)

*Goal*: Convert to Create ML-compatible format

*Implementation*:
```swift
func convertToCreateMLFormat(_ examples: [TrainingExample]) -> CreateMLDataset {
    // Create ML expects CSV or JSON with specific schema
    return examples.map { example in
        CreateMLExample(
            text: example.originalText,
            label: formatLabel(
                quantity: example.correctQuantity,
                unit: example.correctUnit,
                ingredient: example.correctIngredient
            )
        )
    }
}
```

*Output*:
- `train.csv`: Training data for Create ML
- `validate.csv`: Validation data
- `test.csv`: Test data

---

### Phase 2: Model Training (6-8 hours)

**M9.5.5: Create ML Project Setup** (1 hour)

*Goal*: Configure Create ML project

*Steps*:
1. Open Create ML app (included with Xcode)
2. Create new **Text Classifier** project
3. Name: "IngredientParser"
4. Load training data (`train.csv`)
5. Load validation data (`validate.csv`)
6. Configure:
   - Algorithm: Transfer Learning (BERT)
   - Max iterations: 100
   - Validation split: 15%

*Documentation*:
```
Create ML Configuration:
- Project Type: Text Classifier
- Algorithm: Transfer Learning (BERT)
- Training Data: 210 examples
- Validation Data: 45 examples
- Target: 99%+ validation accuracy
```

---

**M9.5.6: Model Training & Tuning** (3-5 hours)

*Goal*: Train model to 99%+ accuracy

*Process*:
1. **Initial Training** (30 min)
   - Start with default settings
   - Monitor accuracy during training
   - Record baseline: ~95-97% validation accuracy

2. **Hyperparameter Tuning** (2-3 hours)
   - Adjust max iterations (50, 100, 200)
   - Try different algorithms (Maximum Entropy, Transfer Learning)
   - Experiment with validation split
   - Target: 99%+ validation accuracy

3. **Overfitting Check** (30 min)
   - Compare train vs validate accuracy
   - If train >> validate: Reduce complexity
   - If both similar: Good generalization

4. **Test Set Evaluation** (30 min)
   - Load `test.csv` (never seen by model)
   - Evaluate final accuracy
   - Target: ‚â•99% on test set

*Output*:
- `IngredientParser.mlmodel`: Trained CoreML model
- `training-metrics.json`: Accuracy, loss, confusion matrix
- `test-results.json`: Performance on held-out data

---

**M9.5.7: Model Export & Integration** (1 hour)

*Goal*: Export model for Xcode integration

*Steps*:
1. Export `.mlmodel` file from Create ML
2. Add to Xcode project (drag into project navigator)
3. Xcode auto-generates Swift interface
4. Verify model loads successfully

*Generated Interface*:
```swift
// Auto-generated by Xcode
class IngredientParser {
    let model: MLModel
    
    func prediction(text: String) throws -> IngredientParserOutput {
        let input = IngredientParserInput(text: text)
        return try model.prediction(from: input)
    }
}

struct IngredientParserOutput {
    let label: String  // Predicted structured format
    let labelProbabilities: [String: Double]
}
```

---

**M9.5.8: Model Validation** (1-2 hours)

*Goal*: Verify model works in production

*Testing*:
```swift
class MLModelTests: XCTestCase {
    let model = try! IngredientParser(configuration: .init())
    
    func testSimpleIngredients() {
        let tests = [
            "2 cups flour",
            "1 1/2 tbsp olive oil",
            "3 eggs"
        ]
        
        for text in tests {
            let output = try! model.prediction(text: text)
            XCTAssertGreaterThan(output.labelProbabilities[output.label]!, 0.9)
        }
    }
    
    func testComplexIngredients() {
        // Test patterns from M8 telemetry
        let tests = [
            "2-3 cloves garlic, minced",
            "1 can (14.5 oz) diced tomatoes",
            "1 lb chicken breast, cut into 1-inch cubes"
        ]
        
        for text in tests {
            let output = try! model.prediction(text: text)
            // Verify correct parsing
        }
    }
    
    func testUserCorrections() {
        // Test actual user corrections from telemetry
        let corrections = loadUserCorrections()
        
        var correctCount = 0
        for correction in corrections {
            let output = try! model.prediction(text: correction.original)
            if matches(output, correction.expected) {
                correctCount += 1
            }
        }
        
        let accuracy = Double(correctCount) / Double(corrections.count)
        XCTAssertGreaterThan(accuracy, 0.99)
    }
}
```

---

### Phase 3: On-Device Inference (3-4 hours)

**M9.5.9: ML Parser Implementation** (2 hours)

*Goal*: Create ML-powered parser

*Implementation*:
```swift
class MLIngredientParser: IngredientParser {
    private let model: IngredientParser
    
    init() throws {
        self.model = try IngredientParser(configuration: .init())
    }
    
    func parse(text: String) -> StructuredQuantity {
        let startTime = CFAbsoluteTimeGetCurrent()
        
        do {
            // Get ML prediction
            let output = try model.prediction(text: text)
            
            // Parse structured label
            let components = parseLabel(output.label)
            
            // Get confidence from probabilities
            let confidence = Float(output.labelProbabilities[output.label] ?? 0.0)
            
            let duration = CFAbsoluteTimeGetCurrent() - startTime
            
            return StructuredQuantity(
                numericValue: components.quantity,
                standardUnit: components.unit,
                displayText: text,
                isParseable: components.quantity != nil,
                parseConfidence: confidence
            )
        } catch {
            // Fallback to NLP parser
            print("ML prediction failed: \(error)")
            return nlpFallback.parse(text: text)
        }
    }
    
    private func parseLabel(_ label: String) -> (
        quantity: Double?,
        unit: String?,
        ingredient: String
    ) {
        // Parse structured label format
        // Example: "2.0|cup|flour"
        let parts = label.split(separator: "|")
        
        return (
            quantity: Double(parts[0]),
            unit: parts.count > 1 ? String(parts[1]) : nil,
            ingredient: parts.count > 2 ? String(parts[2]) : String(parts[0])
        )
    }
}
```

*Files Created*:
- `Services/Parsers/MLIngredientParser.swift` (~150 lines)

---

**M9.5.10: Hybrid System Integration** (1-2 hours)

*Goal*: Integrate ML parser into existing hybrid system

*Implementation*:
```swift
class HybridIngredientParser: IngredientParser {
    private let regexParser: RegexIngredientParser
    private let mlParser: MLIngredientParser
    
    func parse(text: String) -> StructuredQuantity {
        // Fast path: Try regex first
        let regexResult = regexParser.parse(text: text)
        
        // If high confidence, return immediately
        if regexResult.parseConfidence >= 0.9 {
            return regexResult
        }
        
        // Smart path: Use ML for medium/low confidence
        let mlResult = mlParser.parse(text: text)
        
        // Return highest confidence result
        return mlResult.parseConfidence > regexResult.parseConfidence 
            ? mlResult 
            : regexResult
    }
}
```

*Performance*:
- Fast path (regex): 80% of inputs, < 0.05s
- ML path: 15% of inputs, < 0.2s
- Edit flow: 0.5% of inputs (down from 2%)

---

### Phase 4: Continuous Learning (2-3 hours)

**M9.5.11: Ongoing Telemetry** (1 hour)

*Goal*: Continue collecting corrections for future retraining

*Implementation*:
```swift
// Enhanced telemetry for ML
struct MLParsingAttempt: Codable {
    let timestamp: Date
    let originalText: String
    let mlPrediction: String
    let mlConfidence: Float
    let wasCorrect: Bool  // Based on user action
    let userCorrection: String?  // If edited
}

class MLTelemetryService {
    func logMLAttempt(
        text: String,
        prediction: String,
        confidence: Float,
        wasEdited: Bool,
        correction: String?
    ) {
        let attempt = MLParsingAttempt(
            timestamp: Date(),
            originalText: text,
            mlPrediction: prediction,
            mlConfidence: confidence,
            wasCorrect: !wasEdited,
            userCorrection: correction
        )
        
        appendToMLLog(attempt)
    }
}
```

---

**M9.5.12: Model Retraining Pipeline** (1-2 hours)

*Goal*: Enable periodic model updates

*Implementation*:
```swift
class ModelRetrainingPipeline {
    func shouldRetrain() -> Bool {
        // Retrain if:
        // 1. ‚â•50 new user corrections collected
        // 2. OR ‚â•30 days since last retrain
        // 3. OR accuracy dropped below 99%
        
        let newCorrections = getNewCorrections()
        let daysSinceRetrain = getDaysSinceLastRetrain()
        let currentAccuracy = getCurrentAccuracy()
        
        return newCorrections >= 50 
            || daysSinceRetrain >= 30 
            || currentAccuracy < 0.99
    }
    
    func retrain() async throws {
        // 1. Collect new training data
        let newExamples = collectNewCorrections()
        
        // 2. Merge with existing dataset
        let fullDataset = mergeWithExisting(newExamples)
        
        // 3. Re-split and export
        let (train, validate, test) = splitDataset(fullDataset)
        exportForCreateML(train, validate, test)
        
        // 4. Notify developer to retrain in Create ML
        // (Automated training requires additional setup)
        notifyDeveloper("New training data ready")
    }
}
```

*Process*:
1. App collects corrections automatically
2. Monthly check: `if shouldRetrain() { retrain() }`
3. Developer re-trains in Create ML (10 min)
4. New model deployed via app update
5. Users get improved accuracy automatically

---

## Technical Requirements

### TR-1: Privacy Compliance

**Requirement**: All processing on-device, no cloud inference

**Implementation**:
- CoreML model runs locally
- No network calls for inference
- Training data anonymized (no user IDs)
- Users can opt-out of telemetry

**Validation**:
```swift
XCTAssertNoNetworkCalls(during: {
    _ = mlParser.parse(text: "2 cups flour")
})
```

### TR-2: Performance Budget

**Requirement**: ML inference ‚â§ 0.2s

**Implementation**:
- Model optimization (quantization)
- Batch predictions when possible
- Async prediction for non-blocking

**Validation**:
```swift
measure {
    for text in testInputs {
        _ = try! model.prediction(text: text)
    }
}
// Average < 0.2s per input
```

### TR-3: Model Size

**Requirement**: Model file ‚â§ 5MB

**Implementation**:
- Use transfer learning (smaller than training from scratch)
- Quantize weights (16-bit ‚Üí 8-bit)
- Prune unnecessary features

**Validation**:
```swift
let modelURL = Bundle.main.url(forResource: "IngredientParser", withExtension: "mlmodel")!
let size = try! FileManager.default.attributesOfItem(atPath: modelURL.path)[.size] as! Int64
XCTAssertLessThan(size, 5 * 1024 * 1024)  // 5MB
```

---

## Risk Management

### Technical Risks

**Risk 1: Insufficient training data**
- *Likelihood*: Medium
- *Mitigation*: Need ‚â•100 corrections, wait if needed
- *Validation*: Check telemetry count before starting M9.5

**Risk 2: Model overfits to training data**
- *Likelihood*: Medium
- *Mitigation*: Proper train/validate/test split, regularization
- *Validation*: Test set accuracy within 2% of validation accuracy

**Risk 3: Inference too slow**
- *Likelihood*: Low
- *Mitigation*: Model optimization, async prediction
- *Validation*: Comprehensive performance testing

**Risk 4: Time overrun (> 20 hours)**
- *Likelihood*: Medium
- *Mitigation*: This is optional‚Äîcan abandon if taking too long
- *Validation*: Track time per phase, stop if exceeded budget

### ROI Risks

**Risk 1: Marginal improvement not worth effort**
- *Likelihood*: Medium
- *Impact*: 15-20 hours for +1.5% accuracy
- *Mitigation*: Make it optional, evaluate after M8.0
- *Decision Point*: If M8.0 achieving 98.5%+, maybe skip M9.5

---

## Timeline & Estimation

### Phase-by-Phase Estimates

| Phase | Tasks | Time Estimate |
|-------|-------|---------------|
| **Phase 1** | Training Dataset Creation | **4-5 hours** |
| M9.5.1 | Data collection | 2 hours |
| M9.5.2 | Data labeling & validation | 1 hour |
| M9.5.3 | Dataset splitting | 30 min |
| M9.5.4 | Create ML format conversion | 30 min |
| **Phase 2** | Model Training | **6-8 hours** |
| M9.5.5 | Create ML project setup | 1 hour |
| M9.5.6 | Model training & tuning | 3-5 hours |
| M9.5.7 | Model export & integration | 1 hour |
| M9.5.8 | Model validation | 1-2 hours |
| **Phase 3** | On-Device Inference | **3-4 hours** |
| M9.5.9 | ML parser implementation | 2 hours |
| M9.5.10 | Hybrid system integration | 1-2 hours |
| **Phase 4** | Continuous Learning | **2-3 hours** |
| M9.5.11 | Ongoing telemetry | 1 hour |
| M9.5.12 | Model retraining pipeline | 1-2 hours |

**Total**: 15-20 hours

**Dependencies**: M8 complete, ‚â•100 user corrections collected

---

## Integration Points

### Dependencies

**From M8.0 (Parsing Improvements):**
- Hybrid parser architecture (easy to add ML)
- Telemetry infrastructure collecting corrections
- 100+ user corrections from M8 external beta

**To M9.1-9.4 (Health & Nutrition):**
- Improved parsing enables better nutrition analysis
- Clean ingredient data = accurate nutritional lookups
- Foundation for health-aware recommendations

---

## Future Vision

### Potential Enhancements

**Multi-Language Support**:
- Train separate models for Spanish, French, etc.
- Or use multilingual BERT for transfer learning
- Expand to international users

**Explainability**:
- Show why model made prediction
- Build trust with users
- Debug model failures

**Transfer Learning**:
- Fine-tune larger pre-trained models
- Leverage recipe datasets from web
- Even higher accuracy (99.7%+)

**Federated Learning**:
- Learn from all users' corrections
- Without centralizing data
- Privacy-preserving collective improvement

---

## Post-Milestone Documentation

### After M9.5 Completion (If Pursued)

**Required Documentation Updates:**
1. **Learning Note**: Create `docs/learning-notes/27-m9.5-ml-powered-parsing.md`
   - Document Create ML training process
   - Share model architecture decisions
   - Record accuracy metrics
   - Note continuous learning pipeline

2. **Model Card**: Create `docs/ml-docs/ingredient-parser-model-card.md`
   - Model architecture
   - Training data description
   - Performance metrics
   - Limitations and biases

3. **Update current-story.md**: Mark M9.5 complete with actual hours

4. **Update roadmap.md**: Update M9.5 status, reference learning note

5. **Git Commit**: "M9.5 COMPLETE: ML-powered parsing, 99.5%+ accuracy achieved"

---

## Acceptance Criteria

### Must Have (P0) - Blocking release

- [ ] Training dataset: ‚â•100 labeled examples
- [ ] Test set accuracy: ‚â•99%
- [ ] Model file size: ‚â§5MB
- [ ] Inference time: <0.2s (p95)
- [ ] Overall parsing accuracy: ‚â•99.5%
- [ ] User edit rate: <0.5%
- [ ] Privacy: All on-device inference
- [ ] Zero data leaks
- [ ] Build succeeds with no warnings
- [ ] All tests passing

### Should Have (P1) - Strong preference

- [ ] Continuous learning pipeline functional
- [ ] Model retraining process documented
- [ ] A/B testing shows improvement
- [ ] User feedback positive
- [ ] Actual time: 15-20 hours
- [ ] Complete documentation

### Nice to Have (P2) - Future enhancement

- [ ] Multi-language support
- [ ] Explainability features
- [ ] Federated learning foundation
- [ ] Transfer learning experiments

---

## Decision Point: Should You Do M9.5?

### ‚úÖ Pursue M9.5 If:

- [ ] You have 100+ user corrections from M7-M8
- [ ] M8.0 accuracy is 97-98% (room for improvement)
- [ ] You want to showcase ML expertise in portfolio
- [ ] You have 15-20 hours available
- [ ] You're aiming for "best-in-class" product
- [ ] You enjoy ML experimentation

### ‚ùå Skip M9.5 If:

- [ ] M8.0 accuracy is already 98.5%+ (diminishing returns)
- [ ] You have limited time (<20 hours available)
- [ ] You're satisfied with "professional-grade" accuracy
- [ ] You'd rather invest in other features (M10-M12)
- [ ] Your user base is small (<100 active users)
- [ ] Telemetry shows <50 corrections so far

**There is no wrong answer.** M8.0's 98% accuracy is already professional-grade. M9.5 is purely about achieving industry-leading excellence.

---

## Getting Started (If Pursuing)

### Pre-Development Checklist

**Before M9.5.1:**
- [ ] Read session-startup-checklist.md
- [ ] Read project-naming-standards.md
- [ ] Read current-story.md
- [ ] Read M9.5 PRD (this document) completely
- [ ] Verify M8 completely finished
- [ ] Have ‚â•100 user corrections in telemetry
- [ ] Have 15-20 hours available
- [ ] Confirmed decision to pursue M9.5

### First Session Start Prompt

```
I'm ready to start M9.5 - ML-Powered Parsing (OPTIONAL).

I've completed:
‚úÖ session-startup-checklist.md
‚úÖ project-naming-standards.md
‚úÖ current-story.md
‚úÖ M9.5 PRD review
‚úÖ M8 validation complete
‚úÖ 100+ user corrections collected
‚úÖ Confirmed decision to pursue M9.5

Let's start with M9.5.1: Data Collection.

Current state:
- M8.0 hybrid NLP parser achieving 98% accuracy
- Telemetry contains 120+ user corrections
- Ready to extract training dataset
- Create ML app installed and ready

First task: Parse telemetry file, extract user corrections,
create training dataset in Create ML-compatible format.
```

---

**Version**: 1.0  
**Status**: ‚è≥ PLANNED (OPTIONAL)  
**Estimated**: 15-20 hours  
**Dependencies**: M8 complete with ‚â•100 user corrections  
**Decision Point**: Evaluate ROI after M8.0 complete  
**Next Milestone**: M10 Budget Intelligence (or skip to M10)

---

## üéØ FINAL RECOMMENDATION

**M9.5 is a "stretch goal"** - pursue only if:
1. M8.0 shows clear room for improvement (95-97% accuracy)
2. You have substantial training data (100+ corrections)
3. You want to showcase ML expertise
4. You have time available (15-20 hours)

**Otherwise**: M8.0's hybrid NLP system (98%+ accuracy) is **professional-grade and sufficient** for a production app. Focus efforts on other high-value features (M10-M12).
